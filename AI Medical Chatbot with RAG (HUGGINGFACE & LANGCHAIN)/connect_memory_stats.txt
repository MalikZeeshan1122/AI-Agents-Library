LLM Connection Statistics Report
==============================
Generated on: 2024-03-23 14:35:00

Model Configuration
-----------------
1. Large Language Model
   - Model: mistralai/Mistral-7B-Instruct-v0.3
   - Temperature: 0.5
   - Max Output Length: 512 tokens
   - Provider: HuggingFace

2. Embedding Model
   - Model: sentence-transformers/all-MiniLM-L6-v2
   - Vector Dimension: 384
   - Provider: HuggingFace

Vector Store Configuration
------------------------
- Storage Type: FAISS
- Location: vectorstore/db_faiss
- Total Chunks Available: 41,255
- Retrieval Configuration: k=3 (top 3 similar documents)

Query Processing Setup
--------------------
- Custom Prompt Template: Configured
- Chain Type: stuff
- Return Source Documents: Enabled
- Context Window: 3 documents per query

Performance Settings
------------------
- Similarity Search: Enabled
- Safe Deserialization: Enabled
- Temperature: 0.5 (balanced creativity/consistency)
- Response Format: Direct answers without small talk

System Configuration
------------------
- Operating System: Windows 10
- Python Environment: 3.13
- Required APIs: HuggingFace
- Memory Usage: Optimized for retrieval

Usage Instructions
----------------
1. Input: Plain text questions
2. Output Format:
   - Direct answer from context
   - Source documents reference
3. Response Time: ~2-3 seconds per query
4. Context Limitation: 3 most relevant documents

Notes
-----
- Model is optimized for medical domain queries
- Responses are constrained to provided context
- System will acknowledge when answer is not found
- Source documents are provided for verification 